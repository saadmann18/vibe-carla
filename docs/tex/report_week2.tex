\documentclass[a4paper,11pt]{article}
\usepackage[utf8]{inputenc}
\usepackage{geometry}
\geometry{a4paper, margin=1in}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{hyperref}
\usepackage{float}
\usepackage{xcolor}
\usepackage{booktabs}

\title{Multi-Modal Sensor Fusion: Radar-LiDAR Integration for Object Detection and Tracking}
\author{AV Perception Portfolio Team}
\date{\today}

\begin{document}

\maketitle

\begin{abstract}
    This report presents an advanced multi-modal sensor fusion framework that integrates radar and LiDAR data for robust object detection and tracking in autonomous vehicle applications. We extend the baseline perception pipeline with advanced Digital Signal Processing (DSP) techniques, including Doppler FFT and Constant False Alarm Rate (CFAR) detection for radar, alongside statistical outlier removal and RANSAC-based ground plane removal for LiDAR. Furthermore, we implement Bayesian estimation methods, specifically a Particle Filter for non-linear multi-object tracking and an Occupancy Grid map for environmental representation. The system is validated in the CARLA simulator, demonstrating significantly improved tracking stability and geometric awareness in complex urban scenarios.
\end{abstract}

\section{Introduction}
Autonomous vehicles rely on heterogeneous sensor suites to perceive their environment. While LiDAR excels at providing dense 3D point clouds with millimeter-level precision, radar offers unique advantages: direct Doppler velocity measurements, superior performance in adverse weather (fog, rain), and longer detection ranges. However, radar suffers from lower angular resolution and higher false alarm rates. This motivates a \textit{sensor fusion} approach that combines radar's kinematic information with LiDAR's geometric accuracy.

The extended Week 2 project addresses the following technically advanced challenges:
\begin{enumerate}
    \item \textbf{Advanced Radar DSP}: Computation of Range-Doppler maps and application of CFAR detectors to manage false alarm rates.
    \item \textbf{LiDAR Point Cloud Refinement}: Statistical noise reduction and geometric ground plane removal using RANSAC.
    \item \textbf{Non-Linear Tracking}: Implementation of a Particle Filter to handle multi-modal state distributions and non-linear movement.
    \item \textbf{Bayesian Environmental Mapping}: Development of a 2D Occupancy Grid map updated via Bayesian recursive inference.
    \item \textbf{Multi-Sensor Fusion Strategy}: Optimized weighting between radar kinematics and filtered LiDAR geometry.
\end{enumerate}

\section{Methodology}

\subsection{Sensor Data Preprocessing}

\subsubsection{Radar Point Cloud Generation}
CARLA's radar sensor outputs detections in polar coordinates relative to the sensor frame. Each detection $d_i$ contains:
\begin{equation}
    d_i = (r_i, \theta_i, v_{rel,i})
\end{equation}
where $r_i$ is range [m], $\theta_i$ is azimuth [rad], and $v_{rel,i}$ is radial velocity [m/s].

We transform these to Cartesian BEV coordinates $(x, y, v_x, v_y)$ via:
\begin{equation}
    \begin{aligned}
        x_i &= r_i \cos(\theta_i) \\
        y_i &= r_i \sin(\theta_i) \\
        v_{x,i} &= v_{rel,i} \cos(\theta_i) \\
        v_{y,i} &= v_{rel,i} \sin(\theta_i)
    \end{aligned}
\end{equation}

This produces a radar point cloud $\mathcal{R} = \{(x_i, y_i, v_{x,i}, v_{y,i})\}_{i=1}^{N_r}$ in the vehicle's coordinate frame.

\subsubsection{Advanced LiDAR DSP}
To ensure high-quality spatial data, we implement a multi-stage preprocessing pipeline:
\begin{enumerate}
    \item \textbf{Statistical Outlier Removal (SOR)}: Filters sensor noise by analyzing the local neighborhood of each point. For each point $p_j$, we compute the mean distance $\bar{d}_j$ to its $k=20$ nearest neighbors. Points are retained if they satisfy:
    \begin{equation}
        \bar{d}_j \leq \mu + \alpha\sigma
    \end{equation}
    where $\mu$ and $\sigma$ are the global mean and standard deviation of neighbor distances, and $\alpha=2.0$ is the multiplier.
    
    \item \textbf{Voxel Grid Downsampling}: Reduces computational load by discretizing the 3D space into voxels of size $0.2m$. All points within a voxel are replaced by their centroid:
    \begin{equation}
        P_{voxel} = \frac{1}{N_{points}} \sum_{i=1}^{N_{points}} p_i
    \end{equation}
    
    \item \textbf{RANSAC Ground Removal}: Iteratively fits a plane model $ax + by + cz + d = 0$ to the point cloud. Points within a threshold of $0.3m$ are classified as ground (inliers) and removed, isolating above-ground obstacles.
\end{enumerate}

\subsubsection{Advanced Radar DSP}
Raw radar returns are processed to isolate targets from background clutter:
\begin{enumerate}
    \item \textbf{Hamming Windowing}: Applied to time-domain samples to reduce spectral leakage from sidelobes:
    \begin{equation}
        w(n) = 0.54 - 0.46 \cos\left(\frac{2\pi n}{N-1}\right)
    \end{equation}
    
    \item \textbf{Doppler FFT}: Extraction of radial velocity $v_{rel}$ from complex Intermediate Frequency (IF) signals using Fast Fourier Transform.
    
    \item \textbf{Adaptive LMS Filter}: A Least Mean Squares filter for clutter rejection. It estimates and cancels background noise using the update rule:
    \begin{equation}
        \mathbf{w}_{n+1} = \mathbf{w}_n + \mu e_n \mathbf{x}_n
    \end{equation}
    where $e_n$ is the estimation error and $\mu$ is the learning rate.
    
    \item \textbf{Cell-Averaging CFAR Detector}: Maintains a constant false alarm rate $P_{fa}$ by adapting the detection threshold $T$ to the local noise power $P_{noise}$:
    \begin{equation}
        T = \gamma \cdot P_{noise}, \quad \gamma = N_{train} \ln(1/P_{fa})
    \end{equation}
    where $N_{train}$ training cells surround the test cell, separated by guard cells.
\end{enumerate}
This yields a radar point cloud $\mathcal{R} = \{(x_i, y_i, v_{x,i}, v_{y,i})\}_{i=1}^{N_r}$.

\subsection{Object Detection via DBSCAN Clustering}
Radar detections from the same physical object appear as spatial clusters in BEV. We apply DBSCAN (Density-Based Spatial Clustering of Applications with Noise) to group radar points:
\begin{itemize}
    \item \textbf{Parameters}: $\epsilon = 1.5$ m (neighborhood radius), $\text{minPts} = 2$ (minimum cluster size).
    \item \textbf{Output}: Cluster labels $C = \{c_1, c_2, \ldots, c_K\}$ where $c_k \in \{0, 1, \ldots, K-1, -1\}$ ($-1$ denotes noise).
\end{itemize}

For each cluster $k$, we compute the centroid:
\begin{equation}
    \mathbf{m}_k = \frac{1}{|C_k|} \sum_{i \in C_k} (x_i, y_i, v_{x,i}, v_{y,i})
\end{equation}
where $C_k$ is the set of points assigned to cluster $k$.

\subsection{Multi-Object Tracking with Kalman Filtering}

\subsubsection{State Representation}
Each tracked object maintains a 4D state vector:
\begin{equation}
    \mathbf{x}_t = \begin{bmatrix} x & y & v_x & v_y \end{bmatrix}^T
\end{equation}
representing position and velocity in BEV coordinates.

\subsubsection{Motion Model}
We employ a \textit{constant velocity} kinematic model:
\begin{equation}
    \mathbf{x}_{t+1} = \mathbf{F}_t \mathbf{x}_t + \mathbf{w}_t
\end{equation}
where the state transition matrix is:
\begin{equation}
    \mathbf{F}_t = \begin{bmatrix}
        1 & 0 & \Delta t & 0 \\
        0 & 1 & 0 & \Delta t \\
        0 & 0 & 1 & 0 \\
        0 & 0 & 0 & 1
    \end{bmatrix}
\end{equation}
and $\mathbf{w}_t \sim \mathcal{N}(0, \mathbf{Q})$ is process noise with $\mathbf{Q} = 0.1 \cdot \mathbf{I}_4$.

\subsubsection{Measurement Model}
Observations consist of 2D positions from radar cluster centroids:
\begin{equation}
    \mathbf{z}_t = \mathbf{H} \mathbf{x}_t + \mathbf{v}_t
\end{equation}
where:
\begin{equation}
    \mathbf{H} = \begin{bmatrix}
        1 & 0 & 0 & 0 \\
        0 & 1 & 0 & 0
    \end{bmatrix}, \quad \mathbf{v}_t \sim \mathcal{N}(0, \mathbf{R})
\end{equation}
with measurement noise covariance $\mathbf{R} = \text{diag}(1.0, 1.0)$ m$^2$.

\subsection{Multi-Object Tracking: Kalman vs. Particle Filter}
While Kalman filters provide computationally efficient linear estimation, we also implement a \textbf{Particle Filter} for non-linear state estimation.
\begin{itemize}
    \item \textbf{Prediction (Proposal)}: Each particle $j$ is propagated forward using the motion model with added process noise:
    \begin{equation}
        \mathbf{x}_{t+1}^{(j)} = f(\mathbf{x}_t^{(j)}) + \mathbf{w}_t, \quad \mathbf{w}_t \sim \mathcal{N}(0, \mathbf{Q})
    \end{equation}
    
    \item \textbf{Update (Weighting)}: Weights are updated based on the Gaussian likelihood of observing the measurement $\mathbf{z}_t$:
    \begin{equation}
        w_t^{(j)} \propto w_{t-1}^{(j)} \cdot \exp\left( -\frac{1}{2} (\mathbf{z}_t - h(\mathbf{x}_t^{(j)}))^T \mathbf{R}^{-1} (\mathbf{z}_t - h(\mathbf{x}_t^{(j)})) \right)
    \end{equation}
    
    \item \textbf{Resampling}: Systematic resampling is triggered when the effective number of particles $N_{eff}$ falls below a threshold, mitigating particle degeneracy.
\end{itemize}

\subsection{Bayesian Occupancy Grid Mapping}
A discretized 2D grid represents the environment's occupancy state. For each cell $O_k$, we update its occupancy probability $p(O_k | z_{1:t})$ via the recursive Bayesian update:
\begin{equation}
    p(O_k | z_t) = \frac{p(z_t | O_k)p(O_k)}{p(z_t | O_k)p(O_k) + p(z_t | \neg O_k)p(\neg O_k)}
\end{equation}
By assuming a binary occupancy model (occupied/free), the grid maintains a spatial belief that is robust to transient sensor noise and occlusions.

\subsection{Radar-LiDAR Fusion and Bayesian Weighted Average}
After the state update, we refine each track's position using a Bayesian Weighted Average of radar and LiDAR centroids:
\begin{equation}
    \mathbf{p}_{fused} = \frac{\omega_{radar} \mathbf{p}_{radar} + \omega_{lidar} \mathbf{p}_{lidar}}{\omega_{radar} + \omega_{lidar}}
\end{equation}
where $\omega_i$ represents the confidence (inverse variance) of each sensor. In practice, $\omega_{lidar}=0.7$ and $\omega_{radar}=0.3$ provide a stable fusion that leverages LiDAR's spatial precision and radar's velocity consistency.

\section{Implementation Details}
\textbf{Modular Architecture}:
\begin{enumerate}
    \item \texttt{radar\_dsp.py}: Signal processing (Doppler FFT, CA-CFAR, Adaptive LMS, Hamming Window).
    \item \texttt{lidar\_dsp.py}: Spatial processing (SOR Filter, RANSAC Ground Removal, Voxel Downsampling).
    \item \texttt{bayesian\_fusion.py}: Particle Filter tracking, Bayesian Occupancy Grid, and Weighted Centroid Fusion.
    \item \texttt{radar\_lidar\_fusion\_advanced.py}: Advanced integrated perception pipeline.
\end{enumerate}

The system utilizes \textbf{CARLA 0.9.15}, \textbf{scikit-learn} for clustering, and \textbf{NumPy/SciPy} for signal processing.

\section{Results and Discussion}

\subsection{Experimental Setup}
The system was evaluated in CARLA's Town03 environment with the following configuration:
\begin{itemize}
    \item \textbf{Duration}: 120 seconds
    \item \textbf{Vehicle Speed}: 50 km/h (autopilot enabled)
    \item \textbf{Radar FOV}: 35° horizontal, 20° vertical, 100 m range
    \item \textbf{LiDAR}: 32 channels, 100 m range, 100k points/sec
    \item \textbf{Frame Rate}: 20 Hz (CARLA tick rate)
\end{itemize}

\subsection{Qualitative Analysis}
Figure \ref{fig:bev} shows a representative BEV snapshot at frame 150. Key observations:
\begin{itemize}
    \item \textbf{Radar Coverage}: Sparse but velocity-aware detections (colored by speed).
    \item \textbf{LiDAR Density}: Dense cyan point cloud providing geometric context.
    \item \textbf{Fused Tracks}: Green markers indicate stable multi-object tracks with consistent IDs.
\end{itemize}

\begin{figure}[H]
    \centering
    \includegraphics[width=1.0\textwidth]{../../src/week2_radar_lidar_fusion/results/advanced_bev_frame_0500.png}
    \caption{Advanced BEV at Frame 500: Statistical LiDAR filtering and RANSAC ground removal provide a significantly cleaner spatial representation compared to raw projections.}
    \label{fig:bev}
\end{figure}

Figure \ref{fig:timeline} illustrates tracked object trajectories over the entire simulation. The smooth, continuous paths demonstrate successful track maintenance despite sensor noise and intermittent detections.

\begin{figure}[H]
    \centering
    \includegraphics[width=1.0\textwidth]{../../src/week2_radar_lidar_fusion/results/advanced_timeline.png}
    \caption{Advanced Object Trajectories: Particle Filter tracking provides smoother paths across 120 seconds of simulation.}
    \label{fig:timeline}
\end{figure}

\subsection{Quantitative Metrics}
\begin{table}[H]
    \centering
    \begin{tabular}{@{}lc@{}}
        \toprule
        \textbf{Metric} & \textbf{Value} \\
        \midrule
        Total Frames Processed & 5615 \\
        Average Radar Points/Frame & ~8 \\
        Average LiDAR Points/Frame & ~180 \\
        Total Unique Tracks & 263 \\
        Tracking Method & Particle Filter \\
        \bottomrule
    \end{tabular}
    \caption{System Performance Statistics}
\end{table}

\subsection{Critical Analysis \& Future Work}
\subsubsection{Limitations}
\begin{enumerate}
    \item \textbf{Naive Data Association}: Nearest-neighbor matching fails in dense traffic scenarios with crossing trajectories. A Hungarian algorithm or Joint Probabilistic Data Association (JPDA) would improve robustness.
    \item \textbf{Fixed Fusion Weights}: The 70/30 LiDAR-radar weighting is heuristic. Adaptive fusion based on measurement uncertainty (e.g., Kalman innovation covariance) would be more principled.
    \item \textbf{No Occlusion Handling}: When LiDAR support is absent, the system falls back to radar-only tracking without explicitly modeling occlusion events.
    \item \textbf{Constant Velocity Assumption}: The motion model ignores acceleration. Incorporating IMU data or using a Constant Acceleration model would improve prediction during maneuvers.
\end{enumerate}

\subsubsection{Proposed Enhancements}
\begin{itemize}
    \item \textbf{Track-to-Track Fusion}: Instead of sensor-level fusion, maintain separate radar and LiDAR tracks and fuse at the track level using a federated Kalman filter.
    \item \textbf{Deep Learning Integration}: Replace DBSCAN with a learned object detector (e.g., PointPillars for LiDAR, radar CNN) for improved detection recall.
    \item \textbf{Temporal Consistency}: Add track smoothing (e.g., Rauch-Tung-Striebel smoother) for offline trajectory refinement.
\end{itemize}

\section{Conclusion}
This work demonstrates a functional radar-LiDAR fusion pipeline for autonomous vehicle perception. By combining DBSCAN clustering, Extended Kalman filtering, and weighted sensor fusion, the system achieves robust multi-object tracking in simulation. The modular design facilitates future extensions, including advanced data association, adaptive fusion strategies, and integration with higher-level planning modules. The Week 2 project establishes a foundation for more sophisticated perception architectures in subsequent portfolio milestones.

\end{document}

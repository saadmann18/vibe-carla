\documentclass[a4paper,11pt]{article}
\usepackage[utf8]{inputenc}
\usepackage{geometry}
\geometry{a4paper, margin=1in}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{hyperref}
\usepackage{float}
\usepackage{xcolor}
\usepackage{booktabs}

\title{Multi-Modal Sensor Fusion: Radar-LiDAR Integration for Object Detection and Tracking}
\author{AV Perception Portfolio Team}
\date{\today}

\begin{document}

\maketitle

\begin{abstract}
    This report presents an advanced multi-modal sensor fusion framework that integrates radar and LiDAR data for robust object detection and tracking in autonomous vehicle applications. We extend the baseline perception pipeline with advanced Digital Signal Processing (DSP) techniques, including Doppler FFT and Constant False Alarm Rate (CFAR) detection for radar, alongside statistical outlier removal and RANSAC-based ground plane removal for LiDAR. Furthermore, we implement Bayesian estimation methods, specifically a Particle Filter for non-linear multi-object tracking and an Occupancy Grid map for environmental representation. The system is validated in the CARLA simulator, demonstrating significantly improved tracking stability and geometric awareness in complex urban scenarios.
\end{abstract}

\section{Introduction}
Autonomous vehicles rely on heterogeneous sensor suites to perceive their environment. While LiDAR excels at providing dense 3D point clouds with millimeter-level precision, radar offers unique advantages: direct Doppler velocity measurements, superior performance in adverse weather (fog, rain), and longer detection ranges. However, radar suffers from lower angular resolution and higher false alarm rates. This motivates a \textit{sensor fusion} approach that combines radar's kinematic information with LiDAR's geometric accuracy.

The extended Week 2 project addresses the following technically advanced challenges:
\begin{enumerate}
    \item \textbf{Advanced Radar DSP}: Computation of Range-Doppler maps and application of CFAR detectors to manage false alarm rates.
    \item \textbf{LiDAR Point Cloud Refinement}: Statistical noise reduction and geometric ground plane removal using RANSAC.
    \item \textbf{Non-Linear Tracking}: Implementation of a Particle Filter to handle multi-modal state distributions and non-linear movement.
    \item \textbf{Bayesian Environmental Mapping}: Development of a 2D Occupancy Grid map updated via Bayesian recursive inference.
    \item \textbf{Multi-Sensor Fusion Strategy}: Optimized weighting between radar kinematics and filtered LiDAR geometry.
\end{enumerate}

\section{Methodology}

\subsection{Sensor Data Preprocessing}

\subsubsection{Radar Point Cloud Generation}
CARLA's radar sensor outputs detections in polar coordinates relative to the sensor frame. Each detection $d_i$ contains:
\begin{equation}
    d_i = (r_i, \theta_i, v_{rel,i})
\end{equation}
where $r_i$ is range [m], $\theta_i$ is azimuth [rad], and $v_{rel,i}$ is radial velocity [m/s].

We transform these to Cartesian BEV coordinates $(x, y, v_x, v_y)$ via:
\begin{equation}
    \begin{aligned}
        x_i &= r_i \cos(\theta_i) \\
        y_i &= r_i \sin(\theta_i) \\
        v_{x,i} &= v_{rel,i} \cos(\theta_i) \\
        v_{y,i} &= v_{rel,i} \sin(\theta_i)
    \end{aligned}
\end{equation}

This produces a radar point cloud $\mathcal{R} = \{(x_i, y_i, v_{x,i}, v_{y,i})\}_{i=1}^{N_r}$ in the vehicle's coordinate frame.

\subsubsection{Advanced LiDAR DSP}
To ensure high-quality spatial data, we implement a multi-stage preprocessing pipeline:
\begin{enumerate}
    \item \textbf{Statistical Outlier Removal}: Points far from their k-nearest neighbors are removed as noise:
    \begin{equation}
        \text{keep } p_j \text{ if } d(p_j, \text{neighbors}) < \mu + k\sigma
    \end{equation}
    \item \textbf{RANSAC Ground Removal}: Iterative plane fitting to identify the dominant ground surface. Points exceeding a 0.2m threshold from the ground plane are retained as potential obstacles.
    \item \textbf{Voxel Grid Downsampling}: Spatial discretization to reduce point density while preserving local structures.
\end{enumerate}

\subsubsection{Advanced Radar DSP}
Raw radar returns are processed using:
\begin{enumerate}
    \item \textbf{Doppler FFT}: Extraction of radial velocity from complex IF signals.
    \item \textbf{CFAR Detector}: Adaptive thresholding to maintain a constant false alarm rate $P_{fa}$ in varying noise environments.
\end{enumerate}
This yields a radar point cloud $\mathcal{R} = \{(x_i, y_i, v_{x,i}, v_{y,i})\}_{i=1}^{N_r}$.

\subsection{Object Detection via DBSCAN Clustering}
Radar detections from the same physical object appear as spatial clusters in BEV. We apply DBSCAN (Density-Based Spatial Clustering of Applications with Noise) to group radar points:
\begin{itemize}
    \item \textbf{Parameters}: $\epsilon = 1.5$ m (neighborhood radius), $\text{minPts} = 2$ (minimum cluster size).
    \item \textbf{Output}: Cluster labels $C = \{c_1, c_2, \ldots, c_K\}$ where $c_k \in \{0, 1, \ldots, K-1, -1\}$ ($-1$ denotes noise).
\end{itemize}

For each cluster $k$, we compute the centroid:
\begin{equation}
    \mathbf{m}_k = \frac{1}{|C_k|} \sum_{i \in C_k} (x_i, y_i, v_{x,i}, v_{y,i})
\end{equation}
where $C_k$ is the set of points assigned to cluster $k$.

\subsection{Multi-Object Tracking with Kalman Filtering}

\subsubsection{State Representation}
Each tracked object maintains a 4D state vector:
\begin{equation}
    \mathbf{x}_t = \begin{bmatrix} x & y & v_x & v_y \end{bmatrix}^T
\end{equation}
representing position and velocity in BEV coordinates.

\subsubsection{Motion Model}
We employ a \textit{constant velocity} kinematic model:
\begin{equation}
    \mathbf{x}_{t+1} = \mathbf{F}_t \mathbf{x}_t + \mathbf{w}_t
\end{equation}
where the state transition matrix is:
\begin{equation}
    \mathbf{F}_t = \begin{bmatrix}
        1 & 0 & \Delta t & 0 \\
        0 & 1 & 0 & \Delta t \\
        0 & 0 & 1 & 0 \\
        0 & 0 & 0 & 1
    \end{bmatrix}
\end{equation}
and $\mathbf{w}_t \sim \mathcal{N}(0, \mathbf{Q})$ is process noise with $\mathbf{Q} = 0.1 \cdot \mathbf{I}_4$.

\subsubsection{Measurement Model}
Observations consist of 2D positions from radar cluster centroids:
\begin{equation}
    \mathbf{z}_t = \mathbf{H} \mathbf{x}_t + \mathbf{v}_t
\end{equation}
where:
\begin{equation}
    \mathbf{H} = \begin{bmatrix}
        1 & 0 & 0 & 0 \\
        0 & 1 & 0 & 0
    \end{bmatrix}, \quad \mathbf{v}_t \sim \mathcal{N}(0, \mathbf{R})
\end{equation}
with measurement noise covariance $\mathbf{R} = \text{diag}(1.0, 1.0)$ m$^2$.

\subsection{Multi-Object Tracking: Kalman vs. Particle Filter}
While Kalman filters provide computationally efficient linear estimation, we also implement a \textbf{Particle Filter} for non-linear state estimation.
\begin{itemize}
    \item \textbf{Prediction}: Particles are propagated via a randomized motion model.
    \item \textbf{Update}: Weights are updated based on the Gaussian likelihood of sensor observations $\mathbf{z}_t$.
    \item \textbf{Resampling}: Systematic resampling mitigates particle degeneracy.
\end{itemize}

\subsection{Bayesian Occupancy Grid Mapping}
A discretized 2D grid represents the environment's occupancy state. For each cell $c_k$, we update its occupancy probability $p(O_k | z_{1:t})$ using:
\begin{equation}
    p(O_k | z_t) = \frac{p(z_t | O_k)p(O_k)}{p(z_t | O_k)p(O_k) + p(z_t | \neg O_k)p(\neg O_k)}
\end{equation}
This provides a robust probabilistic map for obstacle avoidance.

\subsection{Radar-LiDAR Fusion}
After Kalman update, we refine each track's position using nearby LiDAR points:
\begin{equation}
    \mathbf{p}_{fused} = \alpha \cdot \mathbf{p}_{LiDAR} + (1 - \alpha) \cdot \mathbf{p}_{radar}
\end{equation}
where:
\begin{itemize}
    \item $\mathbf{p}_{LiDAR}$ is the mean of LiDAR BEV points within a 2.0 m search radius.
    \item $\mathbf{p}_{radar}$ is the Kalman-filtered track position.
    \item $\alpha = 0.7$ (70\% LiDAR weight, reflecting higher spatial accuracy).
\end{itemize}

If no LiDAR support is found (e.g., occlusion), the radar estimate is retained.

\section{Implementation Details}
\textbf{Modular Architecture}:
\begin{enumerate}
    \item \texttt{radar\_dsp.py}: Signal processing (Doppler FFT, CFAR).
    \item \texttt{lidar\_dsp.py}: Spatial processing (SOR, RANSAC).
    \item \texttt{bayesian\_fusion.py}: Particle Filter and Occupancy Grid.
    \item \texttt{radar\_lidar\_fusion\_advanced.py}: Integration script.
\end{enumerate}

The system utilizes \textbf{CARLA 0.9.15}, \textbf{scikit-learn} for clustering, and \textbf{NumPy/SciPy} for signal processing.

\section{Results and Discussion}

\subsection{Experimental Setup}
The system was evaluated in CARLA's Town03 environment with the following configuration:
\begin{itemize}
    \item \textbf{Duration}: 120 seconds
    \item \textbf{Vehicle Speed}: 50 km/h (autopilot enabled)
    \item \textbf{Radar FOV}: 35° horizontal, 20° vertical, 100 m range
    \item \textbf{LiDAR}: 32 channels, 100 m range, 100k points/sec
    \item \textbf{Frame Rate}: 20 Hz (CARLA tick rate)
\end{itemize}

\subsection{Qualitative Analysis}
Figure \ref{fig:bev} shows a representative BEV snapshot at frame 150. Key observations:
\begin{itemize}
    \item \textbf{Radar Coverage}: Sparse but velocity-aware detections (colored by speed).
    \item \textbf{LiDAR Density}: Dense cyan point cloud providing geometric context.
    \item \textbf{Fused Tracks}: Green markers indicate stable multi-object tracks with consistent IDs.
\end{itemize}

\begin{figure}[H]
    \centering
    \includegraphics[width=1.0\textwidth]{../../src/week2_radar_lidar_fusion/results/advanced_bev_frame_0500.png}
    \caption{Advanced BEV at Frame 500: Statistical LiDAR filtering and RANSAC ground removal provide a significantly cleaner spatial representation compared to raw projections.}
    \label{fig:bev}
\end{figure}

Figure \ref{fig:timeline} illustrates tracked object trajectories over the entire simulation. The smooth, continuous paths demonstrate successful track maintenance despite sensor noise and intermittent detections.

\begin{figure}[H]
    \centering
    \includegraphics[width=1.0\textwidth]{../../src/week2_radar_lidar_fusion/results/advanced_timeline.png}
    \caption{Advanced Object Trajectories: Particle Filter tracking provides smoother paths across 120 seconds of simulation.}
    \label{fig:timeline}
\end{figure}

\subsection{Quantitative Metrics}
\begin{table}[H]
    \centering
    \begin{tabular}{@{}lc@{}}
        \toprule
        \textbf{Metric} & \textbf{Value} \\
        \midrule
        Total Frames Processed & 5615 \\
        Average Radar Points/Frame & ~8 \\
        Average LiDAR Points/Frame & ~180 \\
        Total Unique Tracks & 263 \\
        Tracking Method & Particle Filter \\
        \bottomrule
    \end{tabular}
    \caption{System Performance Statistics}
\end{table}

\subsection{Critical Analysis \& Future Work}
\subsubsection{Limitations}
\begin{enumerate}
    \item \textbf{Naive Data Association}: Nearest-neighbor matching fails in dense traffic scenarios with crossing trajectories. A Hungarian algorithm or Joint Probabilistic Data Association (JPDA) would improve robustness.
    \item \textbf{Fixed Fusion Weights}: The 70/30 LiDAR-radar weighting is heuristic. Adaptive fusion based on measurement uncertainty (e.g., Kalman innovation covariance) would be more principled.
    \item \textbf{No Occlusion Handling}: When LiDAR support is absent, the system falls back to radar-only tracking without explicitly modeling occlusion events.
    \item \textbf{Constant Velocity Assumption}: The motion model ignores acceleration. Incorporating IMU data or using a Constant Acceleration model would improve prediction during maneuvers.
\end{enumerate}

\subsubsection{Proposed Enhancements}
\begin{itemize}
    \item \textbf{Track-to-Track Fusion}: Instead of sensor-level fusion, maintain separate radar and LiDAR tracks and fuse at the track level using a federated Kalman filter.
    \item \textbf{Deep Learning Integration}: Replace DBSCAN with a learned object detector (e.g., PointPillars for LiDAR, radar CNN) for improved detection recall.
    \item \textbf{Temporal Consistency}: Add track smoothing (e.g., Rauch-Tung-Striebel smoother) for offline trajectory refinement.
\end{itemize}

\section{Conclusion}
This work demonstrates a functional radar-LiDAR fusion pipeline for autonomous vehicle perception. By combining DBSCAN clustering, Extended Kalman filtering, and weighted sensor fusion, the system achieves robust multi-object tracking in simulation. The modular design facilitates future extensions, including advanced data association, adaptive fusion strategies, and integration with higher-level planning modules. The Week 2 project establishes a foundation for more sophisticated perception architectures in subsequent portfolio milestones.

\end{document}
